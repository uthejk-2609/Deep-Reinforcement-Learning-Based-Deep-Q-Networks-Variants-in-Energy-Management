{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "# Define the environment class\n",
    "class RoomEnvironment:\n",
    "    def __init__(self):\n",
    "        self.light_intensity = 10\n",
    "        self.outside_temperature = 25\n",
    "        self.room_temperature = 25\n",
    "        self.fan_speed = 0\n",
    "        self.ac_temperature = 25\n",
    "        self.is_person_present = False\n",
    "\n",
    "    def update_state(self, light_intensity, outside_temperature, room_temperature, fan_speed, ac_temperature, is_person_present):\n",
    "        self.light_intensity = light_intensity\n",
    "        self.outside_temperature = outside_temperature\n",
    "        self.room_temperature = room_temperature\n",
    "        self.fan_speed = fan_speed\n",
    "        self.ac_temperature = ac_temperature\n",
    "        self.is_person_present = is_person_present\n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.light_intensity, self.outside_temperature, self.room_temperature, self.fan_speed, self.ac_temperature, self.is_person_present)\n",
    "\n",
    "    def take_action(self, action):\n",
    "        # Apply the action and update the environment state\n",
    "        if action == 0:\n",
    "            self.ac_temperature -= 1\n",
    "        elif action == 1:\n",
    "            self.ac_temperature += 1\n",
    "        elif action == 2:\n",
    "            self.fan_speed -= 1\n",
    "        elif action == 3:\n",
    "            self.fan_speed += 1\n",
    "\n",
    "        # Only update room temperature if no person is present\n",
    "        if not self.is_person_present:\n",
    "            self.room_temperature += (self.fan_speed - 1) * 0.5 + (self.ac_temperature - 25) * 0.1\n",
    "            # Ensure that room temperature is within a reasonable range (e.g., between 18 and 28 degrees)\n",
    "            self.room_temperature = max(18, min(28, self.room_temperature))\n",
    "\n",
    "        if abs(self.room_temperature - 22) <= 1:\n",
    "            reward = 5\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        return reward\n",
    "\n",
    "# Define the ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add_experience(self, experience):\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DDQN agent class\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_space, action_space, learning_rate, discount_factor, exploration_rate, min_exploration_rate, exploration_decay, replay_buffer_size, batch_size):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.state_size = len(state_space)\n",
    "\n",
    "        self.primary_q_network = self.build_q_network()\n",
    "        self.target_q_network = self.build_q_network()\n",
    "        self.target_q_network.set_weights(self.primary_q_network.get_weights())\n",
    "\n",
    "    def build_q_network(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(450, input_shape=(6,), activation='relu'),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(len(self.action_space), activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state_input = np.array(state).reshape(1, -1)\n",
    "            q_values = self.primary_q_network.predict(state_input)[0]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_q_network(self, state, action, reward, next_state):\n",
    "        experience = (state, action, reward, next_state)\n",
    "        self.replay_buffer.add_experience(experience)\n",
    "\n",
    "        if len(self.replay_buffer.buffer) >= self.batch_size:\n",
    "            batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "\n",
    "            states, actions, rewards, next_states = zip(*batch)\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "\n",
    "            primary_q_values = self.primary_q_network.predict(next_states)\n",
    "            target_q_values = self.target_q_network.predict(next_states)\n",
    "\n",
    "            targets = rewards + self.discount_factor * target_q_values[np.arange(self.batch_size), np.argmax(primary_q_values, axis=1)]\n",
    "            q_values = self.primary_q_network.predict(states)\n",
    "\n",
    "            for i, a in enumerate(actions):\n",
    "                q_values[i][a] = targets[i]\n",
    "\n",
    "            self.primary_q_network.fit(states, q_values, verbose=0)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_q_network.set_weights(self.primary_q_network.get_weights())\n",
    "\n",
    "    def decay_exploration(self):\n",
    "        self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay)\n",
    "\n",
    "# Define the state space, action space, and initialize the environment and the agent\n",
    "state_space = [\n",
    "    (\n",
    "        light_intensity,\n",
    "        outside_temperature,\n",
    "        room_temperature,\n",
    "        fan_speed,\n",
    "        ac_temperature,\n",
    "        is_person_present\n",
    "    )\n",
    "    for light_intensity in range(11)\n",
    "    for outside_temperature in range(15, 36)\n",
    "    for room_temperature in range(15, 36)\n",
    "    for fan_speed in range(6)\n",
    "    for ac_temperature in range(15, 36)\n",
    "    for is_person_present in [False, True]\n",
    "]\n",
    "\n",
    "action_space = [0, 1, 2, 3]\n",
    "environment = RoomEnvironment()\n",
    "ddqn_agent1 = DDQNAgent(state_space, action_space, learning_rate=0.001, discount_factor=0.9, exploration_rate=1.0, min_exploration_rate=0.1, exploration_decay=0.995, replay_buffer_size=1000, batch_size=32)\n",
    "ddqn_agent2 = DDQNAgent(state_space, action_space, learning_rate=0.01, discount_factor=0.9, exploration_rate=1.5, min_exploration_rate=0.1, exploration_decay=0.995, replay_buffer_size=1000, batch_size=64)\n",
    "ddqn_agent3 = DDQNAgent(state_space, action_space, learning_rate=0.01, discount_factor=0.9, exploration_rate=1.5, min_exploration_rate=0.1, exploration_decay=0.995, replay_buffer_size=2000, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDQN Agent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "number_of_episodes = 10\n",
    "\n",
    "rewards_per_episode1 = []\n",
    "mae_loss_per_episode1 = []  # List to store MAE loss per episode\n",
    "\n",
    "for episode in range(number_of_episodes):  # 5 episodes\n",
    "    state = environment.get_state()\n",
    "    total_reward = 0\n",
    "    total_mae_loss = 0\n",
    "    \n",
    "    for step in range(100):  # 100 steps per episode\n",
    "        action = ddqn_agent1.get_action(state)\n",
    "        reward = environment.take_action(action)\n",
    "        next_state = environment.get_state()\n",
    "        ddqn_agent1.update_q_network(state, action, reward, next_state)\n",
    "        \n",
    "        # Calculate MSE loss and update total loss\n",
    "        mae_loss = np.mean(np.abs(ddqn_agent1.primary_q_network.predict(np.array([state])) - ddqn_agent1.target_q_network.predict(np.array([state]))))\n",
    "        total_mae_loss += mae_loss\n",
    "        \n",
    "        ddqn_agent1.decay_exploration()\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        sleep(0.0001)\n",
    "        print('Episode:', episode, 'Step:', step, 'Action:', action, 'Reward:', reward, 'Exploration rate:', ddqn_agent1.exploration_rate, end='\\r')\n",
    "        \n",
    "    ddqn_agent1.update_target_network()  # Update target network at the end of each episode\n",
    "    rewards_per_episode1.append(total_reward)\n",
    "    mae_loss_per_episode1.append(total_mae_loss/100)\n",
    "    print(\"Total Reward:\", total_reward, \"MAE Loss:\", total_mae_loss, \"Episode\", episode, end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDQN Agent 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "rewards_per_episode2 = []\n",
    "mae_loss_per_episode2 = []  # List to store MAE loss per episode\n",
    "\n",
    "for episode in range(number_of_episodes):  # 5 episodes\n",
    "    state = environment.get_state()\n",
    "    total_reward = 0\n",
    "    total_mae_loss = 0\n",
    "    \n",
    "    for step in range(100):  # 100 steps per episode\n",
    "        action = ddqn_agent2.get_action(state)\n",
    "        reward = environment.take_action(action)\n",
    "        next_state = environment.get_state()\n",
    "        ddqn_agent2.update_q_network(state, action, reward, next_state)\n",
    "        \n",
    "        # Calculate MSE loss and update total loss\n",
    "        mae_loss = np.mean(np.abs(ddqn_agent2.primary_q_network.predict(np.array([state])) - ddqn_agent2.target_q_network.predict(np.array([state]))))\n",
    "        total_mae_loss += mae_loss\n",
    "        \n",
    "        ddqn_agent2.decay_exploration()\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        sleep(0.0001)\n",
    "        print('Episode:', episode, 'Step:', step, 'Action:', action, 'Reward:', reward, 'Exploration rate:', ddqn_agent2.exploration_rate, end='\\r')\n",
    "        \n",
    "    ddqn_agent2.update_target_network()  # Update target network at the end of each episode\n",
    "    rewards_per_episode2.append(total_reward)\n",
    "    mae_loss_per_episode2.append(total_mae_loss/100)\n",
    "    print(\"Total Reward:\", total_reward, \"MAE Loss:\", total_mae_loss, \"Episode\", episode, end='\\r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDQN Agent 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "rewards_per_episode3 = []\n",
    "mae_loss_per_episode3 = []  # List to store MAE loss per episode\n",
    "\n",
    "for episode in range(10):  # 5 episodes\n",
    "    state = environment.get_state()\n",
    "    total_reward = 0\n",
    "    total_mae_loss = 0\n",
    "    \n",
    "    for step in range(100):  # 100 steps per episode\n",
    "        action = ddqn_agent3.get_action(state)\n",
    "        reward = environment.take_action(action)\n",
    "        next_state = environment.get_state()\n",
    "        ddqn_agent3.update_q_network(state, action, reward, next_state)\n",
    "        \n",
    "        # Calculate MSE loss and update total loss\n",
    "        mae_loss = np.mean(np.abs(ddqn_agent3.primary_q_network.predict(np.array([state])) - ddqn_agent3.target_q_network.predict(np.array([state]))))\n",
    "        total_mae_loss += mae_loss\n",
    "        \n",
    "        ddqn_agent3.decay_exploration()\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        sleep(0.0001)\n",
    "        print('Episode:', episode, 'Step:', step, 'Action:', action, 'Reward:', reward, 'Exploration rate:', ddqn_agent3.exploration_rate, end='\\r')\n",
    "        \n",
    "    ddqn_agent3.update_target_network()  # Update target network at the end of each episode\n",
    "    rewards_per_episode3.append(total_reward)\n",
    "    mae_loss_per_episode3.append(total_mae_loss/100)\n",
    "    print(\"Total Reward:\", total_reward, \"MAE Loss:\", total_mae_loss, \"Episode\", episode, end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all the three agents reward per episode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards_per_episode1, label='DDQN Agent 1', color='red')\n",
    "plt.plot(rewards_per_episode2, label='DDQN Agent 2', color='blue')\n",
    "plt.plot(rewards_per_episode3, label='DDQN Agent 3', color='green')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DDQN Training Performance-Reward per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plottting all the three agents MAE loss per episode\n",
    "plt.plot(mae_loss_per_episode1, label='DDQN Agent 1', color='red')\n",
    "plt.plot(mae_loss_per_episode2, label='DDQN Agent 2', color='blue')\n",
    "plt.plot(mae_loss_per_episode3, label='DDQN Agent 3', color='green')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('MAE Loss')\n",
    "plt.title('DDQN Training Performance-MAE Loss per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
