{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "# Define the environment class\n",
    "class RoomEnvironment:\n",
    "    def __init__(self):\n",
    "        self.light_intensity = 0  # Initial light intensity\n",
    "        self.outside_temperature = 25  # Initial outside temperature\n",
    "        self.room_temperature = 25  # Initial room temperature\n",
    "        self.fan_speed = 0  # Initial fan speed\n",
    "        self.ac_temperature = 25  # Initial air conditioner temperature\n",
    "        self.is_person_present = False  # Whether someone is in the room or not\n",
    "\n",
    "    def update_state(self, light_intensity, outside_temperature, room_temperature, fan_speed, ac_temperature, is_person_present):\n",
    "        self.light_intensity = light_intensity\n",
    "        self.outside_temperature = outside_temperature\n",
    "        self.room_temperature = room_temperature\n",
    "        self.fan_speed = fan_speed\n",
    "        self.ac_temperature = ac_temperature\n",
    "        self.is_person_present = is_person_present\n",
    "\n",
    "    def get_state(self):\n",
    "        return (self.light_intensity, self.outside_temperature, self.room_temperature, self.fan_speed, self.ac_temperature, self.is_person_present)\n",
    "\n",
    "    def take_action(self, action):\n",
    "        # Apply the action and update the environment state\n",
    "        if action == 0:  # Decrease AC temperature\n",
    "            self.ac_temperature -= 1\n",
    "        elif action == 1:  # Increase AC temperature\n",
    "            self.ac_temperature += 1\n",
    "        elif action == 2:  # Decrease fan speed\n",
    "            self.fan_speed -= 1\n",
    "        elif action == 3:  # Increase fan speed\n",
    "            self.fan_speed += 1\n",
    "\n",
    "        # Only update room temperature if no person is present\n",
    "        if not self.is_person_present:\n",
    "            self.room_temperature += (self.fan_speed - 1) * 0.5 + (self.ac_temperature - 25) * 0.1\n",
    "            # Ensure that room temperature is within a reasonable range (e.g., between 18 and 28 degrees)\n",
    "            self.room_temperature = max(18, min(28, self.room_temperature))\n",
    "\n",
    "        if abs(self.room_temperature - 22) <= 1:\n",
    "            reward = 5\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        return reward\n",
    "\n",
    "# Define the ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add_experience(self, experience):\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            self.buffer.pop(0)  # Remove oldest experience if the buffer is full\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Agent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space, learning_rate, discount_factor, exploration_rate, min_exploration_rate, exploration_decay, replay_buffer_size, batch_size):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Flatten state space size\n",
    "        self.state_size = len(state_space)\n",
    "        \n",
    "        # Create the Q-network\n",
    "        self.q_network = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(450, input_shape=(6,), activation='relu'),\n",
    "                tf.keras.layers.Dense(256, activation='relu'),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dense(len(action_space), activation='linear')\n",
    "                ])\n",
    "        self.q_network.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state_input = np.array(state).reshape(1, -1)\n",
    "            q_values = self.q_network.predict(state_input)[0]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_q_network(self, state, action, reward, next_state):\n",
    "        experience = (state, action, reward, next_state)\n",
    "        self.replay_buffer.add_experience(experience)\n",
    "\n",
    "        if len(self.replay_buffer.buffer) >= self.batch_size:\n",
    "            batch = self.replay_buffer.sample_batch(self.batch_size)\n",
    "\n",
    "            states, actions, rewards, next_states = zip(*batch)\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "\n",
    "            targets = rewards + self.discount_factor * np.max(self.q_network.predict(next_states), axis=1)\n",
    "            q_values = self.q_network.predict(states)\n",
    "            for i, a in enumerate(actions):\n",
    "                q_values[i][a] = targets[i]\n",
    "\n",
    "            self.q_network.fit(states, q_values, verbose=0)\n",
    "\n",
    "    def decay_exploration(self):\n",
    "        self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay)\n",
    "\n",
    "# Define the state space, action space, and initialize the environment and the agent\n",
    "state_space = [\n",
    "    (\n",
    "        light_intensity,\n",
    "        outside_temperature,\n",
    "        room_temperature,\n",
    "        fan_speed,\n",
    "        ac_temperature,\n",
    "        is_person_present\n",
    "    )\n",
    "    for light_intensity in range(11)\n",
    "    for outside_temperature in range(15, 36)\n",
    "    for room_temperature in range(15, 36)\n",
    "    for fan_speed in range(6)\n",
    "    for ac_temperature in range(15, 36)\n",
    "    for is_person_present in [False, True]\n",
    "]\n",
    "\n",
    "action_space = [0, 1, 2, 3]  # Decrease AC temperature, increase AC temperature, decrease fan speed, increase fan speed\n",
    "environment = RoomEnvironment()\n",
    "\n",
    "agent1 = DQNAgent(state_space, action_space, learning_rate=0.001, discount_factor=0.9, exploration_rate=1.0, min_exploration_rate=0.1, exploration_decay=0.995, replay_buffer_size=1000, batch_size=32)\n",
    "agent2 = DQNAgent(state_space, action_space, learning_rate=0.01, discount_factor=0.9, exploration_rate=1.5, min_exploration_rate=0.1, exploration_decay=0.995, replay_buffer_size=1000, batch_size=64)\n",
    "agent3 = DQNAgent(state_space, action_space, learning_rate=0.01, discount_factor=0.9, exploration_rate=1.5, min_exploration_rate=0.1, exploration_decay=0.995, replay_buffer_size=2000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode1 = []  # List to store rewards per episode\n",
    "mae_loss_per_episode1 = []  # List to store MAE loss per episode\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 10\n",
    "for episode in range(num_episodes):  # 10 episodes\n",
    "    state = environment.get_state()\n",
    "    episode_exploration_rate = []\n",
    "    total_reward = 0\n",
    "    total_mae_loss = 0\n",
    "    \n",
    "    for step in range(100):  # 100 steps per episode\n",
    "        action = agent1.get_action(state)\n",
    "        reward = environment.take_action(action)\n",
    "        next_state = environment.get_state()\n",
    "        agent1.update_q_network(state, action, reward, next_state)\n",
    "        \n",
    "        # Calculate MAE loss and accumulate for the episode\n",
    "        state_input = np.array(state).reshape(1, -1)\n",
    "        target = np.array([reward + agent1.discount_factor * np.max(agent1.q_network.predict(np.array([next_state])))])\n",
    "\n",
    "        mae_loss = np.mean(np.abs(target - agent1.q_network.predict(state_input)))\n",
    "        total_mae_loss += mae_loss\n",
    "        \n",
    "        state = next_state\n",
    "        agent1.decay_exploration()\n",
    "        total_reward += reward\n",
    "        sleep(0.0001)\n",
    "        print('Episode:', episode, 'Step:', step, 'Action:', action, 'Reward:', reward, 'Exploration rate:', agent1.exploration_rate, end='\\r')\n",
    "        \n",
    "    rewards_per_episode1.append(total_reward)\n",
    "    mae_loss_per_episode1.append(total_mae_loss/100)\n",
    "    print(\"Total reward:\", total_reward, \"MAE Loss:\", total_mae_loss, \"after training loop\", \"Episode\", episode, end='\\r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode2 = []  # List to store rewards per episode\n",
    "mae_loss_per_episode2 = []  # List to store MAE loss per episode\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):  # 10 episodes\n",
    "    state = environment.get_state()\n",
    "    episode_exploration_rate = []\n",
    "    total_reward = 0\n",
    "    total_mae_loss = 0\n",
    "    \n",
    "    for step in range(100):  # 100 steps per episode\n",
    "        action = agent2.get_action(state)\n",
    "        reward = environment.take_action(action)\n",
    "        next_state = environment.get_state()\n",
    "        agent2.update_q_network(state, action, reward, next_state)\n",
    "        \n",
    "        # Calculate MAE loss and accumulate for the episode\n",
    "        state_input = np.array(state).reshape(1, -1)\n",
    "        target = np.array([reward + agent1.discount_factor * np.max(agent1.q_network.predict(np.array([next_state])))])\n",
    "\n",
    "        mae_loss = np.mean(np.abs(target - agent1.q_network.predict(state_input)))/10\n",
    "        total_mae_loss += mae_loss\n",
    "        \n",
    "        state = next_state\n",
    "        agent2.decay_exploration()\n",
    "        total_reward += reward\n",
    "        sleep(0.0001)\n",
    "        print('Episode:', episode, 'Step:', step, 'Action:', action, 'Reward:', reward, 'Exploration rate:', agent2.exploration_rate, end='\\r')\n",
    "        \n",
    "    rewards_per_episode2.append(total_reward)\n",
    "    mae_loss_per_episode2.append(total_mae_loss)\n",
    "    print(\"Total reward:\", total_reward, \"MAE Loss:\", total_mae_loss, \"after training loop\", \"Episode\", episode, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Agent3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode3 = []  # List to store rewards per episode\n",
    "mae_loss_per_episode3 = []  # List to store MAE loss per episode\n",
    "\n",
    "# Training loop\n",
    "for episode in range(10):  # 10 episodes\n",
    "    state = environment.get_state()\n",
    "    episode_exploration_rate = []\n",
    "    total_reward = 0\n",
    "    total_mae_loss = 0\n",
    "    \n",
    "    for step in range(100):  # 100 steps per episode\n",
    "        action = agent3.get_action(state)\n",
    "        reward = environment.take_action(action)\n",
    "        next_state = environment.get_state()\n",
    "        agent3.update_q_network(state, action, reward, next_state)\n",
    "        \n",
    "        # Calculate MAE loss and accumulate for the episode\n",
    "        state_input = np.array(state).reshape(1, -1)\n",
    "        target = np.array([reward + agent1.discount_factor * np.max(agent1.q_network.predict(np.array([next_state])))])\n",
    "\n",
    "        mae_loss = np.mean(np.abs(target - agent1.q_network.predict(state_input)))/10\n",
    "        total_mae_loss += mae_loss\n",
    "        \n",
    "        state = next_state\n",
    "        agent3.decay_exploration()\n",
    "        total_reward += reward\n",
    "        sleep(0.0001)\n",
    "        print('Episode:', episode, 'Step:', step, 'Action:', action, 'Reward:', reward, 'Exploration rate:', agent3.exploration_rate, end='\\r')\n",
    "        \n",
    "    rewards_per_episode3.append(total_reward)\n",
    "    mae_loss_per_episode3.append(total_mae_loss)\n",
    "    print(\"Total reward:\", total_reward, \"MAE Loss:\", total_mae_loss, \"after training loop\", \"Episode\", episode, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all three agents of rewards per episode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards_per_episode1, label='agent1', color='red')\n",
    "plt.plot(rewards_per_episode2, label='agent2', color='blue')\n",
    "plt.plot(rewards_per_episode3, label='agent3', color='green')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN - Total Reward per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all three agents of MAE loss per episode\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.plot(mae_loss_per_episode1, label='agent1', color='red')\n",
    "plt.plot(mae_loss_per_episode2, label='agent2', color='blue')\n",
    "plt.plot(mae_loss_per_episode3, label='agent3', color='green')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('MAE Loss')\n",
    "plt.title('DQN - MAE Loss per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
